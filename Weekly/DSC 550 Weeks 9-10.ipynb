{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1268554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a216b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents(corpus): \n",
    "    return list (corpus.reviews()) \n",
    "def continuous(corpus): \n",
    "    return list(corpus.scores()) \n",
    "def make_categorical(corpus): \n",
    "    \"\"\"terrible : 0.0 < y <= 3.0 okay : 3.0 < y <= 5.0 great : 5.0 < y <= 7.0 amazing : 7.0 < y <= 10.1\"\"\" \n",
    "    return np.digitize(continuous(corpus), [ 0.0 , 3.0 , 5.0 , 7.0 , 10.1 ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf8b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score \n",
    "def train_model(path,model,continuous=True,saveto=None,cv=12): \n",
    "    \"\"\" Trains model from corpus at specified path; constructing cross-validation scores using the cv parameter, then fitting the model on the full data. Returns the scores. \"\"\" \n",
    "    # Load the corpus data and labels for classification \n",
    "    corpus = PickledReviewsReader(path) \n",
    "    X = documents(corpus) \n",
    "    if continuous: \n",
    "        y = continuous(corpus) \n",
    "        scoring = 'r2_score' \n",
    "    else: \n",
    "        y = make_categorical(corpus) \n",
    "        scoring = 'f1_score' \n",
    "    # Compute cross-validation scores \n",
    "    scores = cross_val_score(model,X,y,cv=cv,scoring=scoring) \n",
    "    # Write to disk if specified \n",
    "    if saveto: \n",
    "        joblib.dump(model,saveto) \n",
    "    # Fit the model on entire dataset \n",
    "    model.fit(X,y) \n",
    "    #Return scores \n",
    "    return scores \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "927c579d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextNormalizer' from 'transformers' (/Users/aarondrexler/opt/anaconda3/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7872fcf9d91b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPickledReviewsReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPRegressor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextNormalizer' from 'transformers' (/Users/aarondrexler/opt/anaconda3/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    from transformers import TextNormalizer \n",
    "    from reader import PickledReviewsReader \n",
    "    from sklearn.pipeline import Pipeline \n",
    "    from sklearn.neural_network import MLPRegressor , MLPClassifier \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "    # Path to postpreprocessed, part-of-speech tagged review corpus \n",
    "    cpath = 'controversial-comments.jsonl' \n",
    "    regressor = Pipeline([('norm',TextNormalizer()),('tfidf',TfidfVectorizer()),('ann',MLPRegressor(hidden_layer_sizes=[500,150],verbose = True))]) \n",
    "    regression_scores = train_model(cpath,regressor,continuous=True)\n",
    "    classifier = Pipeline([('norm',TextNormalizer()),('tfidf',TfidfVectorizer()),('ann',MLPClassifier(hidden_layer_sizes=[500,150],verbose = True))]) \n",
    "    classifer_scores = train_model(cpath,classifier,continuous=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45f93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9070cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense \n",
    "from keras.models import Sequential \n",
    "N_FEATURES = 5000 \n",
    "N_CLASSES = 4 \n",
    "def build_network (): \n",
    "    \"\"\" Create a function that returns a compiled neural network \"\"\" \n",
    "    nn = Sequential() \n",
    "    nn.add(Dense(500,activation = 'relu',input_shape=(N_FEATURES,))) \n",
    "    nn.add(Dense(150,activation='relu')) \n",
    "    nn.add(Dense(N_CLASSES,activation='softmax')) \n",
    "    nn.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy']) \n",
    "    return nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5425f6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextNormalizer' from 'transformers' (/Users/aarondrexler/opt/anaconda3/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-05bde9a34648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextNormalizer' from 'transformers' (/Users/aarondrexler/opt/anaconda3/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' : \n",
    "    from sklearn.pipeline import Pipeline \n",
    "    from transformers import TextNormalizer \n",
    "    from keras.wrappers.scikit_learn import KerasClassifier \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "    pipeline = Pipeline ([ ( 'norm' , TextNormalizer ()), ( 'vect' , TfidfVectorizer ( max_features = N_FEATURES )), ( 'nn' , KerasClassifier ( build_fn = build_network , epochs = 200 , batch_size = 128 )) ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f409e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model ( path , model , saveto = None , cv = 12 ): \n",
    "    \"\"\" Trains model from corpus at specified path and fits on full data. If a saveto dictionary is specified, writes Keras and Sklearn pipeline components to disk separately. Returns the scores. \"\"\" \n",
    "    corpus = PickledReviewsReader ( path ) \n",
    "    X = documents ( corpus ) \n",
    "    y = make_categorical ( corpus ) \n",
    "    scores = cross_val_score ( model , X , y , cv = cv , scoring = 'accuracy' , n_jobs =-1 ) \n",
    "    model . fit ( X , y ) \n",
    "    if saveto : \n",
    "        model . steps [ 1 ]\n",
    "        [ 1 ] . model . save ( saveto [ 'keras_model' ]) \n",
    "        model . steps . pop ( 1 ) \n",
    "        joblib . dump ( model , saveto [ 'sklearn_pipe' ]) \n",
    "        return scores \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2803af82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-89c22089138c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'controversial-comments.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'keras_model'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'keras_nn.h5'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'sklearn_pipe'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'pipeline.pkl'\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mcpath\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msaveto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpath\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "cpath = 'controversial-comments.jsonl'  \n",
    "mpath = { 'keras_model' : 'keras_nn.h5' , 'sklearn_pipe' : 'pipeline.pkl' } \n",
    "scores = train_model ( cpath , pipeline , saveto = mpath , cv = 12 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fe82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a83d7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "11501568/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "# Set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64,kernel_size=(5, 5),input_shape=(channels, width, height),activation='relu'))\n",
    "\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "# # Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "## Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "# # Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde00cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
